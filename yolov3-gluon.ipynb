{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "import gluoncv as gcv\n",
    "from gluoncv import data as gdata\n",
    "from gluoncv import utils as gutils\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultTrainTransform\n",
    "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultValTransform\n",
    "from gluoncv.data.dataloader import RandomTransformDataLoader\n",
    "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n",
    "from gluoncv.utils.metrics.coco_detection import COCODetectionMetric\n",
    "from gluoncv.utils import LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset, args, root='~/data/VOC/VOCdevkit'):\n",
    "    if dataset.lower() == 'voc':\n",
    "        train_dataset = gdata.VOCDetection(\n",
    "            root=root, \n",
    "            #splits=[(2007, 'trainval'), (2012, 'trainval')])\n",
    "            splits=[(2007, 'trainval')])\n",
    "        \n",
    "        val_dataset = gdata.VOCDetection(\n",
    "            root=root,\n",
    "            splits=[(2007, 'test')])\n",
    "        \n",
    "        val_metric = VOC07MApMetric(iou_thresh=0.5, class_names=val_dataset.classes)\n",
    "        \n",
    "    elif dataset.lower() == 'coco':\n",
    "        train_dataset = gdata.COCODetection(root=root, splits='instances_train2017', use_crowd=False)\n",
    "        \n",
    "        val_dataset = gdata.COCODetection(root=root, splits='instances_val2017', skip_empty=False)\n",
    "        \n",
    "        val_metric = COCODetectionMetric(\n",
    "            val_dataset, args.save_prefix + '_eval', cleanup=True,\n",
    "            data_shape=(args.data_shape, args.data_shape))\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError('Dataset: {} not implemented.'.format(dataset))\n",
    "        \n",
    "    if args.num_samples < 0:\n",
    "        args.num_samples = len(train_dataset)\n",
    "    if args.mixup:\n",
    "        from gluoncv.data import MixupDetection\n",
    "        train_dataset = MixupDetection(train_dataset)\n",
    "    return train_dataset, val_dataset, val_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tuple 将各个函数分别顺序作用于对应的元素\n",
    "+ Stack 将参数堆叠\n",
    "+ Pad   按照最长的元素，将较短元素填充补足\n",
    "\n",
    "```python\n",
    "    >>> from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "    \n",
    "    >>> from gluoncv.data import batchify\n",
    "    >>> a = ([1, 2, 3, 4], 0)\n",
    "    >>> b = ([5, 7], 1)\n",
    "    >>> Tuple(batchify.Pad(), batchify.Stack())([a, b])\n",
    "    (\n",
    "     [[1 2 3 4]\n",
    "      [5 7 0 0]]\n",
    "     <NDArray 2x4 @cpu(0)>,\n",
    "     [0. 1.]\n",
    "     <NDArray 2 @cpu(0)>)    \n",
    "    \n",
    "    >>> import numpy as np\n",
    "    >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "    >>> b = np.array([[5, 6, 7, 8], [1, 2, 3, 4]])\n",
    "    >>> Stack()([a, b])\n",
    "    [[[1. 2. 3. 4.]\n",
    "      [5. 6. 7. 8.]]\n",
    "     [[5. 6. 7. 8.]\n",
    "      [1. 2. 3. 4.]]]\n",
    "    <NDArray 2x2x4 @cpu(0)>    \n",
    "    \n",
    "    >>> import numpy as np\n",
    "    >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "    >>> b = np.array([[5, 8], [1, 2]])\n",
    "    >>> Pad(axis=1, pad_val=-1)([a, b])\n",
    "    [[[ 1  2  3  4]\n",
    "      [ 5  6  7  8]]\n",
    "     [[ 5  8 -1 -1]\n",
    "      [ 1  2 -1 -1]]]\n",
    "    <NDArray 2x2x4 @cpu(0)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers, args):\n",
    "    \"\"\"Get dataloader.\"\"\"\n",
    "    width, height = data_shape, data_shape\n",
    " \n",
    "    # return stacked images, center_targets, scale_targets, gradient weights, objectness_targets, class_targets\n",
    "    # additionally, return padded ground truth bboxes, so there are 7 components returned by dataloader\n",
    "    # 图片，中心点坐标，目标宽高，\n",
    "    # stack image, all targets generated\n",
    "    batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(1)]))  \n",
    "    \n",
    "    if args.no_random_shape:\n",
    "        train_loader = gluon.data.DataLoader(\n",
    "            train_dataset.transform(YOLO3DefaultTrainTransform(width, height, net, mixup=args.mixup)),\n",
    "            batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "    else:\n",
    "        transform_fns = [YOLO3DefaultTrainTransform(x * 32, x * 32, net, mixup=args.mixup) for x in range(10, 20)]\n",
    "        \n",
    "        train_loader = RandomTransformDataLoader(\n",
    "            transform_fns, train_dataset, batch_size=batch_size, interval=10, last_batch='rollover',\n",
    "            shuffle=True, batchify_fn=batchify_fn, num_workers=num_workers)\n",
    "        \n",
    "    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "    val_loader = gluon.data.DataLoader(\n",
    "        val_dataset.transform(YOLO3DefaultValTransform(width, height)),\n",
    "        batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO3DefaultTrainTransform\n",
    "\n",
    "YOLOV3PrefetchTargetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "\n",
    "width, height = 416, 416  # resize image to 416x416 after all data augmentation\n",
    "# 内存泄露？？？每次运行都会耗费很多内存\n",
    "train_transform = presets.yolo.YOLO3DefaultTrainTransform(width, height, net)\n",
    "\n",
    "# YOLOv3 默认使用的 transform \n",
    "# random color jittering\n",
    "# random expansion with prob 0.5\n",
    "# random cropping\n",
    "# resize with random interpolation\n",
    "# random horizontal flip\n",
    "# to tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3 默认使用的 transform \n",
    "\n",
    "1. random color jittering\n",
    "1. random expansion with prob 0.5\n",
    "1. random cropping\n",
    "1. resize with random interpolation\n",
    "1. random horizontal flip\n",
    "1. to tensor\n",
    "\n",
    "集成到 class YOLO3DefaultTrainTransform 中实现，当参数 net == None 的时候，返回训练样本和标签两个值，当 net 不为空的时候，返回 objectness, center_targets, scale_targets, weights, class_targets ，其中实现在 class YOLOV3PrefetchTargetGenerator 中，里面实现了根据  `img, feature maps, anchors, Pre-generated x and y offsets, gt_boxes, gt_ids, gt_mixratio(Mixup ratio from 0 to 1) ` 入参生成 training target\n",
    "\n",
    "\n",
    "YOLOV3PrefetchTargetGenerator 实现中只将匹配 Bbox 的 objectness 写为 1 ，否则为 0\n",
    "\n",
    "```Python\n",
    "objectness[b, index, match, 0] = (np_gt_mixratios[b, m, 0] if np_gt_mixratios is not None else 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(net, best_map, current_map, epoch, save_interval, prefix):\n",
    "    current_map = float(current_map)\n",
    "    if current_map > best_map[0]:\n",
    "        best_map[0] = current_map\n",
    "        net.save_parameters('{:s}_best.params'.format(prefix, epoch, current_map))\n",
    "        with open(prefix+'_best_map.log', 'a') as f:\n",
    "            f.write('{:04d}:\\t{:.4f}\\n'.format(epoch, current_map))\n",
    "    if save_interval and epoch % save_interval == 0:\n",
    "        net.save_parameters('{:s}_{:04d}_{:.4f}.params'.format(prefix, epoch, current_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, val_data, ctx, eval_metric):\n",
    "    \"\"\"Test on validation dataset.\"\"\"\n",
    "    eval_metric.reset()\n",
    "    # set nms threshold and topk constraint\n",
    "    net.set_nms(nms_thresh=0.45, nms_topk=400)\n",
    "    mx.nd.waitall()\n",
    "    net.hybridize()\n",
    "    for batch in val_data:\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        det_bboxes = []\n",
    "        det_ids = []\n",
    "        det_scores = []\n",
    "        gt_bboxes = []\n",
    "        gt_ids = []\n",
    "        gt_difficults = []\n",
    "        for x, y in zip(data, label):\n",
    "            # get prediction results\n",
    "            ids, scores, bboxes = net(x)\n",
    "            det_ids.append(ids)\n",
    "            det_scores.append(scores)\n",
    "            # clip to image size\n",
    "            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n",
    "            # split ground truths\n",
    "            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
    "            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
    "            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n",
    "\n",
    "        # update metric\n",
    "        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)\n",
    "    return eval_metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, eval_metric, ctx, args):\n",
    "    \"\"\"Training pipeline\"\"\"\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    if args.no_wd:\n",
    "        for k, v in net.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "            v.wd_mult = 0.0\n",
    "\n",
    "    if args.label_smooth:\n",
    "        net._target_generator._label_smooth = True\n",
    "\n",
    "    if args.lr_decay_period > 0:\n",
    "        lr_decay_epoch = list(range(args.lr_decay_period, args.epochs, args.lr_decay_period))\n",
    "    else:\n",
    "        lr_decay_epoch = [int(i) for i in args.lr_decay_epoch.split(',')]\n",
    "    lr_scheduler = LRScheduler(mode=args.lr_mode,\n",
    "                               baselr=args.lr,\n",
    "                               niters=args.num_samples // args.batch_size,\n",
    "                               nepochs=args.epochs,\n",
    "                               step=lr_decay_epoch,\n",
    "                               step_factor=args.lr_decay, power=2,\n",
    "                               warmup_epochs=args.warmup_epochs)\n",
    "\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd',\n",
    "        {'wd': args.wd, 'momentum': args.momentum, 'lr_scheduler': lr_scheduler},\n",
    "        kvstore='local')\n",
    "\n",
    "    # targets\n",
    "    sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    l1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "    # metrics\n",
    "    obj_metrics = mx.metric.Loss('ObjLoss')\n",
    "    center_metrics = mx.metric.Loss('BoxCenterLoss')\n",
    "    scale_metrics = mx.metric.Loss('BoxScaleLoss')\n",
    "    cls_metrics = mx.metric.Loss('ClassLoss')\n",
    "\n",
    "    # set up logger\n",
    "    logging.basicConfig()\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    log_file_path = args.save_prefix + '_train.log'\n",
    "    log_dir = os.path.dirname(log_file_path)\n",
    "    if log_dir and not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    fh = logging.FileHandler(log_file_path)\n",
    "    logger.addHandler(fh)\n",
    "    logger.info(args)\n",
    "    logger.info('Start training from [Epoch {}]'.format(args.start_epoch))\n",
    "    \n",
    "    best_map = [0]\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.mixup:\n",
    "            # TODO(zhreshold): more elegant way to control mixup during runtime\n",
    "            try:\n",
    "                train_data._dataset.set_mixup(np.random.beta, 1.5, 1.5)\n",
    "            except AttributeError:\n",
    "                train_data._dataset._data.set_mixup(np.random.beta, 1.5, 1.5)\n",
    "            if epoch >= args.epochs - args.no_mixup_epochs:\n",
    "                try:\n",
    "                    train_data._dataset.set_mixup(None)\n",
    "                except AttributeError:\n",
    "                    train_data._dataset._data.set_mixup(None)\n",
    "\n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        mx.nd.waitall()\n",
    "        net.hybridize()\n",
    "        for i, batch in enumerate(train_data):\n",
    "            batch_size = batch[0].shape[0]\n",
    "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            # objectness, center_targets, scale_targets, weights, class_targets\n",
    "            fixed_targets = [gluon.utils.split_and_load(batch[it], ctx_list=ctx, batch_axis=0) for it in range(1, 6)]\n",
    "            gt_boxes = gluon.utils.split_and_load(batch[6], ctx_list=ctx, batch_axis=0)\n",
    "            sum_losses = []\n",
    "            obj_losses = []\n",
    "            center_losses = []\n",
    "            scale_losses = []\n",
    "            cls_losses = []\n",
    "            with autograd.record():\n",
    "                for ix, x in enumerate(data):\n",
    "                    obj_loss, center_loss, scale_loss, cls_loss = net(x, gt_boxes[ix], *[ft[ix] for ft in fixed_targets])\n",
    "                    sum_losses.append(obj_loss + center_loss + scale_loss + cls_loss)\n",
    "                    obj_losses.append(obj_loss)\n",
    "                    center_losses.append(center_loss)\n",
    "                    scale_losses.append(scale_loss)\n",
    "                    cls_losses.append(cls_loss)\n",
    "                autograd.backward(sum_losses)\n",
    "            lr_scheduler.update(i, epoch)\n",
    "            trainer.step(batch_size)\n",
    "            obj_metrics.update(0, obj_losses)\n",
    "            center_metrics.update(0, center_losses)\n",
    "            scale_metrics.update(0, scale_losses)\n",
    "            cls_metrics.update(0, cls_losses)\n",
    "            if args.log_interval and not (i + 1) % args.log_interval:\n",
    "                name1, loss1 = obj_metrics.get()\n",
    "                name2, loss2 = center_metrics.get()\n",
    "                name3, loss3 = scale_metrics.get()\n",
    "                name4, loss4 = cls_metrics.get()\n",
    "                logger.info('[Epoch {}][Batch {}], LR: {:.2E}, Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "                    epoch, i, trainer.learning_rate, batch_size/(time.time()-btic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
    "            btic = time.time()\n",
    "\n",
    "        name1, loss1 = obj_metrics.get()\n",
    "        name2, loss2 = center_metrics.get()\n",
    "        name3, loss3 = scale_metrics.get()\n",
    "        name4, loss4 = cls_metrics.get()\n",
    "        logger.info('[Epoch {}] Training cost: {:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "            epoch, (time.time()-tic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
    "        if not (epoch + 1) % args.val_interval:\n",
    "            # consider reduce the frequency of validation to save time\n",
    "            map_name, mean_ap = validate(net, val_data, ctx, eval_metric)\n",
    "            val_msg = '\\n'.join(['{}={}'.format(k, v) for k, v in zip(map_name, mean_ap)])\n",
    "            logger.info('[Epoch {}] Validation: \\n{}'.format(epoch, val_msg))\n",
    "            current_map = float(mean_ap[-1])\n",
    "        else:\n",
    "            current_map = 0.\n",
    "        save_params(net, best_map, current_map, epoch, args.save_interval, args.save_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    # fix seed for mxnet, numpy and python builtin random generator.\n",
    "    gutils.random.seed(args.seed)\n",
    "\n",
    "    # training contexts\n",
    "    ctx = [mx.gpu(int(i)) for i in args.gpus.split(',') if i.strip()]\n",
    "    ctx = ctx if ctx else [mx.cpu()]\n",
    "\n",
    "    # network\n",
    "    net_name = '_'.join(('yolo3', args.network, args.dataset))\n",
    "    args.save_prefix += net_name\n",
    "    # use sync bn if specified\n",
    "    if args.syncbn and len(ctx) > 1:\n",
    "        net = get_model(net_name, pretrained_base=True, norm_layer=gluon.contrib.nn.SyncBatchNorm,\n",
    "                        norm_kwargs={'num_devices': len(ctx)})\n",
    "        async_net = get_model(net_name, pretrained_base=False)  # used by cpu worker\n",
    "    else:\n",
    "        net = get_model(net_name, pretrained_base=True)\n",
    "        async_net = net\n",
    "    if args.resume.strip():\n",
    "        net.load_parameters(args.resume.strip())\n",
    "        async_net.load_parameters(args.resume.strip())\n",
    "    else:\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\")\n",
    "            net.initialize()\n",
    "            async_net.initialize()\n",
    "\n",
    "    # training data\n",
    "    train_dataset, val_dataset, eval_metric = get_dataset(args.dataset, args)\n",
    "    train_data, val_data = get_dataloader(\n",
    "        async_net, train_dataset, val_dataset, args.data_shape, args.batch_size, args.num_workers, args)\n",
    "\n",
    "    # training\n",
    "    train(net, train_data, val_data, eval_metric, ctx, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
