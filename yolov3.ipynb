{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "\n",
    "from mxnet.gluon import nn \n",
    "\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet.gluon.data.vision import transforms as gtransforms\n",
    "\n",
    "import gluoncv as gcv\n",
    "from gluoncv import model_zoo \n",
    "from gluoncv import data as gcvdata\n",
    "from gluoncv.utils import TrainingHistory\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设备个数\n",
    "gpu_num = 1\n",
    "ctx = [mx.gpu(i) for i in range(gpu_num)]\n",
    "\n",
    "batch_size_per_pgu = 8\n",
    "batch_size = batch_size_per_pgu * gpu_num\n",
    "\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意将模型加载到 GPU 而不是 CPU\n",
    "#yolov3_model = model_zoo.get_model('yolo3_darknet53_coco', pretrained=True, ctx=ctx)\n",
    "\n",
    "yolov3_model = model_zoo.get_model('yolo3_darknet53_voc', pretrained=True, ctx=ctx)\n",
    "\n",
    "## 冻结原有层的权重\n",
    "#resnet18_cifar10[0].collect_params().setattr('grad_req', 'null')\n",
    "## 初始化自定义层的权重\n",
    "#resnet18_cifar10[1].initialize(init=mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有模型需要加载到 GPU ，训练样本和测试样本并不需要加载到 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#gcvdata.COCODetection(\n",
    "#    [\"root='~/.mxnet/datasets/coco'\", \"splits=('instances_val2017',)\", 'transform=None', 'min_object_area=0', 'skip_empty=True', 'use_crowd=True'],\n",
    "#)\n",
    "\n",
    "coco_train = gcvdata.COCODetection(root='~/data/coco', splits=['instances_train2017'], transform=None)\n",
    "\n",
    "coco_val = gcvdata.COCODetection(root='~/data/coco', splits=['instances_val2017'], transform=None)\n",
    "\n",
    "# train_dataset = gcvdata.COCOInstance(root='~/data/coco', splits=['instances_val2017'], transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = gcvdata.VOCDetection(root='~/data/VOC/VOCdevkit', \n",
    "                                 #splits=[(2007, 'trainval'), (2012, 'trainval')], \n",
    "                                 splits=[(2007, 'trainval')],\n",
    "                                 transform=None)\n",
    "\n",
    "voc_val = gcvdata.VOCDetection(root='~/data/VOC/VOCdevkit',\n",
    "                           splits=[(2007, 'test')],\n",
    "                           transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "\n",
    "transforms_train = presets.yolo.YOLO3DefaultTrainTransform(416,416,yolov3_model)\n",
    "transforms_val = presets.yolo.YOLO3DefaultValTransform(416,416,yolov3_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = gtransforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    gtransforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    gtransforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    gtransforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    gtransforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    gtransforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    gtransforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "transform_val = gtransforms.Compose([\n",
    "    gtransforms.Resize(32),\n",
    "    gtransforms.ToTensor(),\n",
    "    gtransforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gluoncv 也提供的转换的接口\n",
    "from gluoncv.data import transforms as gcvtransforms\n",
    "\n",
    "gcvtransforms.bbox.crop([12,34,56,78])\n",
    "\n",
    "gcvtransforms.block.RandomCrop(32)\n",
    "\n",
    "gcvtransforms.mask.flip([12,34],(2,3))\n",
    "\n",
    "gcvtransforms.image.random_pca_lighting(nd.array([[[1,2,3],[4,5,6]], [[7,8,9],[10,11,12]]]), 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 类提供了两个转换函数： transform_first 和 transform ；\n",
    "# transform_first 只变换 data ； transform 同时变换样本和标签（一个样本的所有数据）\n",
    "\n",
    "loader_train = gdata.DataLoader(voc_train.transform(transforms_train), \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              last_batch='discard', \n",
    "                              num_workers=num_workers)\n",
    "\n",
    "loader_val = gdata.DataLoader(voc_val.transform(transforms_val),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False, \n",
    "                            num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优化方法，学习速率调节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mx.optimizer.Adam(learning_rate=0.0001,\n",
    "                             beta1=0.9,\n",
    "                             beta2=0.999,\n",
    "                             epsilon=1e-08,\n",
    "                             lazy_update=True)\n",
    "\n",
    "trainer = gluon.Trainer(yolov3_model.collect_params(), optimizer)\n",
    "\n",
    "\n",
    "# 调节学习速率衰减的倍数\n",
    "lr_decay = 0.1\n",
    "lr_decay_epochs_set = set([200, 400])\n",
    "\n",
    "def update_learn_rate(trainer, epoch, lr_decay_epochs_set, lr_decay=0.1):\n",
    "    if epoch in lr_decay_epochs_set:\n",
    "        trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_history = TrainingHistory(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluoncv as gcv\n",
    "yolov3_loss = gcv.loss.YOLOV3Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个参数是 pred ，第二个参数是 label\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def metric_val(net, dataset_val, metric=None, ctx=mx.cpu(0)):\n",
    "    \n",
    "    if metric is None:\n",
    "        metric = mx.metric.Accuracy()\n",
    "        \n",
    "    for _, batch in enumerate(dataset_val):\n",
    "        # 将数据切片分别加载到不同的设备上\n",
    "        data_val = gluon.utils.split_and_load(batch[0], ctx, batch_axis=0)\n",
    "        label_val = gluon.utils.split_and_load(batch[1], ctx, batch_axis=0)\n",
    "        \n",
    "        yhat = [net(x) for x in data_val]\n",
    "        \n",
    "        # 第一个参数是 label ， 第二个参数是 output\n",
    "        metric.update(label_val, yhat)    \n",
    "  \n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "\n",
    "# return stacked images, center_targets, scale_targets, gradient weights, objectness_targets, class_targets\n",
    "# additionally, return padded ground truth bboxes, so there are 7 components returned by dataloader\n",
    "batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(1)]))\n",
    "\n",
    "loader_train = gdata.DataLoader(voc_train.transform(transforms_train), \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True,\n",
    "                              batchify_fn=batchify_fn,\n",
    "                              last_batch='discard', \n",
    "                              num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(loader_train):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data 0:', batch[0][0].shape, 'label 0:', batch[1][0].shape)\n",
    "    print('data 1:', batch[0][1].shape, 'label 1:', batch[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(loader_train):\n",
    "    if ib > 0:\n",
    "        break\n",
    "    print('data:', batch[0][0].shape)\n",
    "    print('label:', batch[6][0].shape)\n",
    "    # 将数据切片分别加载到不同的设备上 ； 始终卡在这里，下一条打印信息没有出来\n",
    "    data_train = gluon.utils.split_and_load(batch, ctx_list=ctx, batch_axis=0)\n",
    "    \n",
    "    print(\"aaa\")\n",
    "\n",
    "    with autograd.record():\n",
    "        input_order = [0, 6, 1, 2, 3, 4, 5]\n",
    "        obj_loss, center_loss, scale_loss, cls_loss = yolov3_model(*[data_train[o] for o in input_order])\n",
    "        # sum up the losses\n",
    "        # some standard gluon training steps:\n",
    "        # autograd.backward(sum_loss)\n",
    "        # trainer.step(batch_size)\n",
    "        print(\"bbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, data_train, data_val, trainer, epochs):\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #metric.reset()\n",
    "        train_loss = 0\n",
    "        update_learn_rate(trainer, epoch, lr_decay_epochs, lr_decay)\n",
    "        tic = time.time()\n",
    "        \n",
    "        # dataset 是可迭代对象\n",
    "        for i, batch in enumerate(loader_train):\n",
    "            # 将数据切片分别加载到不同的设备上\n",
    "            data_train = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_train = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            # 自动求导，是因为这里的处理才导致 list 有 backward 函数？\n",
    "            with autograd.record():\n",
    "                output = [net(X) for X in data_train]\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(output, label_train)]                \n",
    "\n",
    "\n",
    "            for l in loss:\n",
    "                # loss 是一个 list ，为什么可以迭代？ 而且这里的 l 和 loss 是一样的？ \n",
    "                l.backward()\n",
    "            \n",
    "            # 这里更新的时候是怎样用到上面计算的 loss 的？\n",
    "            # trainer 已经中指定了需要训练的参数\n",
    "            trainer.step(batch_size)\n",
    "                        \n",
    "            for l in loss:\n",
    "                train_loss += l.sum().asscalar() / batch_size\n",
    "            \n",
    "            # 每个 batch 更新一下训练的准确率\n",
    "            metric.update(label_train, output)\n",
    "            \n",
    "        _, acc = metric.get()\n",
    "        _, val_acc = metric_val(net, dataset_val, ctx=ctx)\n",
    "        \n",
    "        # 这里记录的是错误率\n",
    "        train_history.update([1-acc, 1-val_acc])\n",
    "        \n",
    "        toc = time.time()\n",
    "        \n",
    "        print('[epoch %d] train_loss=%f, acc=%f, val_acc=%f, lr=%.9f, time: %fs' % \n",
    "              (epoch, train_loss, acc, val_acc, trainer.learning_rate, toc-tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net(resnet20, dataset_train, dataset_val, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet20.save_parameters('cifar10_resnet20v2.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet20.load_parameters('cifar10_resnet20v2.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signature:      loss(*args)\n",
    "Type:           YOLOV3Loss\n",
    "String form:    YOLOV3Loss(batch_axis=0, w=None)\n",
    "File:           ~/anaconda3/lib/python3.7/site-packages/gluoncv/loss.py\n",
    "Source:\n",
    "    \n",
    "class YOLOV3Loss(gluon.loss.Loss):\n",
    "    \"\"\"Losses of YOLO v3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_axis : int, default 0\n",
    "        The axis that represents mini-batch.\n",
    "    weight : float or None\n",
    "        Global scalar weight for loss.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_axis=0, weight=None, **kwargs):\n",
    "        super(YOLOV3Loss, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self._sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "        self._l1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "    def hybrid_forward(self, F, objness, box_centers, box_scales, cls_preds,\n",
    "                       objness_t, center_t, scale_t, weight_t, class_t, class_mask):\n",
    "        \"\"\"Compute YOLOv3 losses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        objness : mxnet.nd.NDArray\n",
    "            Predicted objectness (B, N), range (0, 1).\n",
    "        box_centers : mxnet.nd.NDArray\n",
    "            Predicted box centers (x, y) (B, N, 2), range (0, 1).\n",
    "        box_scales : mxnet.nd.NDArray\n",
    "            Predicted box scales (width, height) (B, N, 2).\n",
    "        cls_preds : mxnet.nd.NDArray\n",
    "            Predicted class predictions (B, N, num_class), range (0, 1).\n",
    "        objness_t : mxnet.nd.NDArray\n",
    "            Objectness target, (B, N), 0 for negative 1 for positive, -1 for ignore.\n",
    "        center_t : mxnet.nd.NDArray\n",
    "            Center (x, y) targets (B, N, 2).\n",
    "        scale_t : mxnet.nd.NDArray\n",
    "            Scale (width, height) targets (B, N, 2).\n",
    "        weight_t : mxnet.nd.NDArray\n",
    "            Loss Multipliers for center and scale targets (B, N, 2).\n",
    "        class_t : mxnet.nd.NDArray\n",
    "            Class targets (B, N, num_class).\n",
    "            It's relaxed one-hot vector, i.e., (1, 0, 1, 0, 0).\n",
    "            It can contain more than one positive class.\n",
    "        class_mask : mxnet.nd.NDArray\n",
    "            0 or 1 mask array to mask out ignored samples (B, N, num_class).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of NDArrays\n",
    "            obj_loss: sum of objectness logistic loss\n",
    "            center_loss: sum of box center logistic regression loss\n",
    "            scale_loss: sum of box scale l1 loss\n",
    "            cls_loss: sum of per class logistic loss\n",
    "\n",
    "        \"\"\"\n",
    "        # compute some normalization count, except batch-size\n",
    "        denorm = F.cast(\n",
    "            F.shape_array(objness_t).slice_axis(axis=0, begin=1, end=None).prod(), 'float32')\n",
    "        weight_t = F.broadcast_mul(weight_t, objness_t)\n",
    "        hard_objness_t = F.where(objness_t > 0, F.ones_like(objness_t), objness_t)\n",
    "        new_objness_mask = F.where(objness_t > 0, objness_t, objness_t >= 0)\n",
    "        obj_loss = F.broadcast_mul(\n",
    "            self._sigmoid_ce(objness, hard_objness_t, new_objness_mask), denorm)\n",
    "        center_loss = F.broadcast_mul(self._sigmoid_ce(box_centers, center_t, weight_t), denorm * 2)\n",
    "        scale_loss = F.broadcast_mul(self._l1_loss(box_scales, scale_t, weight_t), denorm * 2)\n",
    "        denorm_class = F.cast(\n",
    "            F.shape_array(class_t).slice_axis(axis=0, begin=1, end=None).prod(), 'float32')\n",
    "        class_mask = F.broadcast_mul(class_mask, objness_t)\n",
    "        cls_loss = F.broadcast_mul(self._sigmoid_ce(cls_preds, class_t, class_mask), denorm_class)\n",
    "        return obj_loss, center_loss, scale_loss, cls_loss\n",
    "    \n",
    "    \n",
    "Call docstring: Calls forward. Only accepts positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "def yolov3loss(objectness_p, box_p, class_p,\n",
    "              objectness_t, box_t, class_t):\n",
    "    '''Compute YOLOv3 losses\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    objectness_p : mxnet.nd.NDArray\n",
    "        Predicted objectness (B, N), range (0, 1).\n",
    "    box_centers : mxnet.nd.NDArray\n",
    "        Predicted box centers (x, y) (B, N, 2), range (0, 1).\n",
    "    box_scales : mxnet.nd.NDArray\n",
    "        Predicted box scales (width, height) (B, N, 2).\n",
    "    cls_preds : mxnet.nd.NDArray\n",
    "        Predicted class predictions (B, N, num_class), range (0, 1).\n",
    "    objness_t : mxnet.nd.NDArray\n",
    "        Objectness target, (B, N), 0 for negative 1 for positive, -1 for ignore.\n",
    "    center_t : mxnet.nd.NDArray\n",
    "        Center (x, y) targets (B, N, 2).\n",
    "    scale_t : mxnet.nd.NDArray\n",
    "        Scale (width, height) targets (B, N, 2).\n",
    "    weight_t : mxnet.nd.NDArray\n",
    "        Loss Multipliers for center and scale targets (B, N, 2).\n",
    "    class_t : mxnet.nd.NDArray\n",
    "        Class targets (B, N, num_class).\n",
    "        It's relaxed one-hot vector, i.e., (1, 0, 1, 0, 0).\n",
    "        It can contain more than one positive class.\n",
    "    class_mask : mxnet.nd.NDArray\n",
    "        0 or 1 mask array to mask out ignored samples (B, N, num_class).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of NDArrays\n",
    "        obj_loss: sum of objectness logistic loss\n",
    "        center_loss: sum of box center logistic regression loss\n",
    "        scale_loss: sum of box scale l1 loss\n",
    "        cls_loss: sum of per class logistic loss   \n",
    "    '''\n",
    "    \n",
    "    l1_loss = gluon.loss.L1Loss()\n",
    "    sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntryLoss(from_sigmoid=False)\n",
    "    \n",
    "    def SSE(lnum, rnum):\n",
    "        assert(lnum.shape == rnum.shape)        \n",
    "        loss = (lnum - rnum) ** 2\n",
    "    \n",
    "    box_reg_loss = SSE(box_p, box_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "def yolov3loss(objectness_p, box_p, class_p,\n",
    "              objectness_t, box_t, class_t):\n",
    "    '''Compute YOLOv3 losses\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    objectness_p : mxnet.nd.NDArray\n",
    "        Predicted objectness (B, N), range (0, 1).\n",
    "    box_centers : mxnet.nd.NDArray\n",
    "        Predicted box centers (x, y) (B, N, 2), range (0, 1).\n",
    "    box_scales : mxnet.nd.NDArray\n",
    "        Predicted box scales (width, height) (B, N, 2).\n",
    "    cls_preds : mxnet.nd.NDArray\n",
    "        Predicted class predictions (B, N, num_class), range (0, 1).\n",
    "    objness_t : mxnet.nd.NDArray\n",
    "        Objectness target, (B, N), 0 for negative 1 for positive, -1 for ignore.\n",
    "    center_t : mxnet.nd.NDArray\n",
    "        Center (x, y) targets (B, N, 2).\n",
    "    scale_t : mxnet.nd.NDArray\n",
    "        Scale (width, height) targets (B, N, 2).\n",
    "    weight_t : mxnet.nd.NDArray\n",
    "        Loss Multipliers for center and scale targets (B, N, 2).\n",
    "    class_t : mxnet.nd.NDArray\n",
    "        Class targets (B, N, num_class).\n",
    "        It's relaxed one-hot vector, i.e., (1, 0, 1, 0, 0).\n",
    "        It can contain more than one positive class.\n",
    "    class_mask : mxnet.nd.NDArray\n",
    "        0 or 1 mask array to mask out ignored samples (B, N, num_class).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of NDArrays\n",
    "        obj_loss: sum of objectness logistic loss\n",
    "        center_loss: sum of box center logistic regression loss\n",
    "        scale_loss: sum of box scale l1 loss\n",
    "        cls_loss: sum of per class logistic loss   \n",
    "    '''\n",
    "    \n",
    "    l1_loss = gluon.loss.L1Loss()\n",
    "    sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntryLoss(from_sigmoid=False)\n",
    "    \n",
    "    def SSE(lnum, rnum):\n",
    "        assert(lnum.shape == rnum.shape)        \n",
    "        loss = (lnum - rnum) ** 2\n",
    "    \n",
    "    box_reg_loss = SSE(box_p, box_t)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
