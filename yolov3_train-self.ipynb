{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载必备库文件\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon.data.vision import transforms as gtransforms\n",
    "\n",
    "from gluoncv import model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 步骤\n",
    "# 1. 读取样本和标注\n",
    "# 1. transform\n",
    "# 1. mini-batch 样本迭代器\n",
    "# 1. 定义 loss 以及 metric\n",
    "# 1. 定义网络结构并初始化权重\n",
    "# 1. 确定最优化方法\n",
    "# 1. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 损失函数\n",
    "l1_loss = gluon.loss.L1Loss()\n",
    "sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "# 第一个参数是 pred ，第二个参数是 label\n",
    "softmax_ce = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 性能度量函数\n",
    "def metric_val(net, dataset_val, metric=None, ctx=mx.cpu(0)):\n",
    "    \n",
    "    if metric is None:\n",
    "        metric = mx.metric.Accuracy()\n",
    "        \n",
    "    for _, batch in enumerate(dataset_val):\n",
    "        # 将数据切片分别加载到不同的设备上\n",
    "        data_val = gluon.utils.split_and_load(batch[0], ctx, batch_axis=0)\n",
    "        label_val = gluon.utils.split_and_load(batch[1], ctx, batch_axis=0)\n",
    "        \n",
    "        yhat = [net(x) for x in data_val]\n",
    "        \n",
    "        # 第一个参数是 label ， 第二个参数是 output\n",
    "        metric.update(label_val, yhat)    \n",
    "  \n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.utils.metrics.coco_detection import COCODetectionMetric\n",
    "COCODetectionMetric??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "presets.yolo.YOLO3DefaultTrainTransform??\n",
    "presets.rcnn.FasterRCNNDefaultTrainTransform??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据预处理\n",
    "## 牢记 mxnet 使用 BCHW 形式\n",
    "\n",
    "# Dataset 类提供了两个转换函数： transform_first 和 transform ；\n",
    "# transform_first 只变换 data ； transform 同时变换样本和标签（一个样本的所有数据）\n",
    "\n",
    "# imgnet 数据集\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 写两个函数，一个是 train_transform 一个 test_transform\n",
    "def train_transform(img, label, mean, std):\n",
    "    \"\"\"\n",
    "    YOLOv3 默认的数据预处理，图像和标签都需要处理\n",
    "    1. random color jittering\n",
    "    1. random expansion with prob 0.5\n",
    "    1. random cropping\n",
    "    1. resize with random interpolation\n",
    "    1. random horizontal flip\n",
    "    1. to tensor\n",
    "    1. nomalize\n",
    "    \"\"\"\n",
    "    \n",
    "    # random color jittering\n",
    "    mx.nd.image.random_color_jitter()\n",
    "    \n",
    "    # random expansion with prob 0.5\n",
    "    \n",
    "    # random cropping\n",
    "    mx.image.fixed_crop()\n",
    "    mx.image.random_crop()\n",
    "    \n",
    "    # resize with random interpolation\n",
    "    mx.image.imresize() # interp 利用入参进行了设置 (img, width, height, interp=interp)\n",
    "    \n",
    "    # random horizontal flip\n",
    "    mx.nd.image.random_flip_left_right()\n",
    "    \n",
    "    # to tensor WHC -> CHW\n",
    "    img = mx.nd.image.to_tensor(img)\n",
    "    \n",
    "    # nomalize\n",
    "    img = mx.nd.image.normalize(img, mean, std)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def test_transform(img, lable, mean, std):\n",
    "    \n",
    "    # resize with random interpolation\n",
    "    mx.image.imresize() # interp 利用入参进行了设置 (img, width, height, interp=interp)\n",
    "    \n",
    "    # to tensor WHC -> CHW\n",
    "    img = mx.nd.image.to_tensor(img)\n",
    "    \n",
    "    # nomalize\n",
    "    img = mx.nd.image.normalize(img, mean, std)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "# gluoncv 代码实现\n",
    "def transform(src, label, width, height, mean, std):\n",
    "    \"\"\"Apply transform to training image/label.\"\"\"\n",
    "    # random color jittering\n",
    "    img = experimental.image.random_color_distort(src)\n",
    "\n",
    "    # random expansion with prob 0.5\n",
    "    if np.random.uniform(0, 1) > 0.5:\n",
    "        img, expand = timage.random_expand(img, fill=[m * 255 for m in mean])\n",
    "        bbox = tbbox.translate(label, x_offset=expand[0], y_offset=expand[1])\n",
    "    else:\n",
    "        img, bbox = img, label\n",
    "\n",
    "    # random cropping\n",
    "    h, w, _ = img.shape\n",
    "    bbox, crop = experimental.bbox.random_crop_with_constraints(bbox, (w, h))\n",
    "    x0, y0, w, h = crop\n",
    "    img = mx.image.fixed_crop(img, x0, y0, w, h)\n",
    "\n",
    "    # resize with random interpolation\n",
    "    h, w, _ = img.shape\n",
    "    interp = np.random.randint(0, 5)\n",
    "    img = timage.imresize(img, width, height, interp=interp)\n",
    "    bbox = tbbox.resize(bbox, (w, h), (width, height))\n",
    "\n",
    "    # random horizontal flip\n",
    "    h, w, _ = img.shape\n",
    "    img, flips = timage.random_flip(img, px=0.5)\n",
    "    bbox = tbbox.flip(bbox, (w, h), flip_x=flips[0])\n",
    "\n",
    "    # to tensor\n",
    "    img = mx.nd.image.to_tensor(img)\n",
    "    img = mx.nd.image.normalize(img, mean=mean, std=std)\n",
    "\n",
    "    return img, bbox.astype(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据加载\n",
    "# from mxnet.gluon.data import DataLoader\n",
    "# 这个函数最主要实现的目的就是每次返回 batch_size 大小的样本 [ Loads data from a dataset and returns mini-batches of data. ]\n",
    "# 阅读 DataLoader 代码发现，这个函数就是先依据入参生成所有样本 batch_sampler (经过 shuffle 或者顺序读取)，\n",
    "# 然后生成一个迭代器，每次返回 batch_size 大小的样本，且会将这些样本进行函数 batchify_fn 处理 \n",
    "# 当然还可以使用多个线程同时读取，可以预先读取一定数量的样本等等\n",
    "# 可以自己去实现，但是感觉没有必要\n",
    "# 参数\n",
    "#    dataset : ndarray or numpy array\n",
    "#    batch_size : int\n",
    "#    shuffle : bool\n",
    "#    sampler : Sampler\n",
    "#    last_batch : {'keep', 'discard', 'rollover'}\n",
    "#    batch_sampler : Sampler\n",
    "#    batchify_fn : callable. 用户自定义组装样本的方法\n",
    "#    num_workers : int, default 0. 使用 num_workers 个线程来读取样本\n",
    "#    pin_memory : boolean, default False. 使用函数 mxnet.ndarray.ndarray.NDArray.as_in_context(context) 实现，加快从 CPU 到 GPU 的拷贝速度\n",
    "#    prefetch : int, default is `num_workers * 2`. 预处理样本的个数，会消耗较大的 shared_memory （应该是 GPU 的），当 num_workers > 0 时生效\n",
    "\n",
    "loader_train = DataLoader(voc_train.transform(transforms_train), \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              last_batch='discard', \n",
    "                              num_workers=num_workers)\n",
    "\n",
    "loader_val = DataLoader(voc_val.transform(transforms_val),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False, \n",
    "                            num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(loader_train):\n",
    "    if ib > 0:\n",
    "        break\n",
    "    print('data:', batch[0][0].shape)\n",
    "    print('label:', batch[6][0].shape)\n",
    "    # 将数据切片分别加载到不同的设备上 ； 始终卡在这里，下一条打印信息没有出来\n",
    "    data_train = gluon.utils.split_and_load(batch, ctx_list=ctx, batch_axis=0)\n",
    "    \n",
    "    print(\"aaa\")\n",
    "\n",
    "    with autograd.record():\n",
    "        input_order = [0, 6, 1, 2, 3, 4, 5]\n",
    "        obj_loss, center_loss, scale_loss, cls_loss = yolov3_model(*[data_train[o] for o in input_order])\n",
    "        # sum up the losses\n",
    "        # some standard gluon training steps:\n",
    "        # autograd.backward(sum_loss)\n",
    "        # trainer.step(batch_size)\n",
    "        print(\"bbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import DataLoader\n",
    "DataLoader??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习参数\n",
    "# 调节学习速率衰减的倍数\n",
    "lr_decay = 0.1\n",
    "lr_decay_epochs_set = set([200, 400])\n",
    "\n",
    "def update_learn_rate(trainer, epoch, lr_decay_epochs_set, lr_decay=0.1):\n",
    "    if epoch in lr_decay_epochs_set:\n",
    "        trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "\n",
    "\n",
    "from gluoncv.utils import TrainingHistory\n",
    "train_history = TrainingHistory(['train', 'val'])\n",
    "\n",
    "\n",
    "# 最优化\n",
    "optimizer = mx.optimizer.Adam(learning_rate=0.0001,\n",
    "                             beta1=0.9,\n",
    "                             beta2=0.999,\n",
    "                             epsilon=1e-08,\n",
    "                             lazy_update=True)\n",
    "\n",
    "trainer = gluon.Trainer(yolov3_model.collect_params(), optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "gluon.Trainer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 冻结原有层的权重\n",
    "#resnet18_cifar10[0].collect_params().setattr('grad_req', 'null')\n",
    "## 初始化自定义层的权重\n",
    "#resnet18_cifar10[1].initialize(init=mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "## 网络训练\n",
    "def train_net(net, data_train, data_val, trainer, epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #metric.reset()\n",
    "        train_loss = 0\n",
    "        update_learn_rate(trainer, epoch, lr_decay_epochs, lr_decay)\n",
    "        tic = time.time()\n",
    "        \n",
    "        # dataset 是可迭代对象\n",
    "        for i, batch in enumerate(loader_train):\n",
    "            # 将数据切片分别加载到不同的设备上；这里假如有 n 个 GPU ，前面的 DataLoader 中 batch_size 是不是应该是 batch*n ? TODO\n",
    "            data_train = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_train = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            # 自动求导。record() 函数使得 mxnet 记录并计算梯度，需要训练的参数都需要计算梯度\n",
    "            with autograd.record():\n",
    "                output = [net(X) for X in data_train]\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(output, label_train)]\n",
    "\n",
    "            # loss 是 list ，当有多个 loss 的时候，所有的 loss 都需要反向传播\n",
    "            # l 是 mxnet.ndarray.ndarray.NDArray 格式的数据，本身就有 backward() 函数\n",
    "            # 调用 backward() 函数用于计算梯度\n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "            \n",
    "            # 更新参数， 通过调用 allreduce_grads() 和 update() 来实现参数的更新\n",
    "            # 必须在 autograd.backward() 之后，以及 record() 之外调用\n",
    "            # allreduce_grads() 必须在 trainer.update() 之前调用\n",
    "            # 这里更新的时候是怎样用到上面计算的 loss 的？\n",
    "            # trainer 已经中指定了需要训练的参数，应该是可以在某个位置找到这些参数的梯度，从而使用指定的最优化方法来更新参数\n",
    "            trainer.step(batch_size)\n",
    "                        \n",
    "            for l in loss:\n",
    "                train_loss += l.sum().asscalar() / batch_size\n",
    "            \n",
    "            # 每个 batch 更新一下训练的准确率\n",
    "            metric.update(label_train, output)\n",
    "            \n",
    "        _, acc = metric.get()\n",
    "        _, val_acc = metric_val(net, dataset_val, ctx=ctx)\n",
    "        \n",
    "        # 这里记录的是错误率\n",
    "        train_history.update([1-acc, 1-val_acc])\n",
    "        \n",
    "        toc = time.time()\n",
    "        \n",
    "        print('[epoch %d] train_loss=%f, acc=%f, val_acc=%f, lr=%.9f, time: %fs' % \n",
    "              (epoch, train_loss, acc, val_acc, trainer.learning_rate, toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
