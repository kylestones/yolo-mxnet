{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载必备库文件\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "from mxnet.gluon import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.hybridize() 可以为继承 HybridBlock 类的层优化计算性能\n",
    "\n",
    "def cbl_gen(channels, kernel_size, strides, padding):\n",
    "    '''conv-BN-LeakyReLU cell'''\n",
    "    cbl_unit = nn.HybridSequential()\n",
    "    # 所有卷积后面都有 BN ，所以 bias 始终为 False\n",
    "    cbl_unit.add(\n",
    "        nn.Conv2D(channels, kernel_size=kernel_size, strides=strides, padding=padding, groups=1, use_bias=False),\n",
    "        nn.BatchNorm(),\n",
    "        nn.LeakyReLU(0.1)\n",
    "    )\n",
    "    \n",
    "    return cbl_unit\n",
    "\n",
    "\n",
    "# 残差网络需要重新定义前向传播的方式，必须自己定义网络层类\n",
    "class DarknetBasicBlockV3(gluon.HybridBlock):\n",
    "    '''darknetV3 basic block'''\n",
    "    \n",
    "    def __init__(self, channels, **kwargs):        \n",
    "        super(DarknetBasicBlockV3, self).__init__(**kwargs)        \n",
    "        self.body = nn.HybridSequential()\n",
    "        self.body.add(\n",
    "            # 1x1 conv; 看 darknet 中 yolov3.cfg 文件中 1x1 卷积的 padding 也是 1 ？？？ gluoncv 中并没有\n",
    "            cbl_gen(channels, (1,1), (1,1), (0,0)),\n",
    "            # 3x3 conv\n",
    "            cbl_gen(channels*2, (3,3), (1,1), (1,1))        \n",
    "        )\n",
    "    # 需要在 hybrid_forward 函数中添加额外的输入F。由于 MXNet 既有基于命令式编程的 NDArray 类，\n",
    "    # 又有基于符号式编程的 Symbol 类。由于这两个类的函数基本一致，MXNet会根据输入来决定 F 使用 NDArray 或 Symbol。    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        return x + self.body(x)\n",
    "\n",
    "        \n",
    "class Darknet53(gluon.HybridBlock):\n",
    "    '''darknet53'''\n",
    "    \n",
    "    def __init__(self, residual_block_num, channels, class_num=1000, **kwargs):\n",
    "        super(Darknet53, self).__init__(**kwargs)\n",
    "        self.features = nn.HybridSequential()\n",
    "        \n",
    "        # 网络最开始有一个卷积操作\n",
    "        self.features.add(cbl_gen(channels[0], (3,3), (1,1), (1,1)))\n",
    "        \n",
    "        # 重复的残差块\n",
    "        for residual_block, channel in zip(residual_block_num, channels[1]):\n",
    "            # 使用步长为 2 的卷积实现下采样，在每一个残差块的开始都有一个下采样层\n",
    "            self.features.add(cbl_gen(channel*2, (3,3), (2,2), (1,1)))\n",
    "            # 一个残差块\n",
    "            for _ in range(residual_block):\n",
    "                self.features.add(DarknetBasicBlockV3(channel))\n",
    "        \n",
    "        # global average pooling\n",
    "        self.pooling = nn.GlobalAvgPool2D()\n",
    "        \n",
    "        # 全连接的输出层\n",
    "        self.output = nn.Dense(class_num)\n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    \n",
    "\n",
    "# default configurations\n",
    "residual_block_num = [1, 2, 8, 8, 4] # 残差块的个数\n",
    "darknet_channels = [32, [32, 64, 128, 256, 512]] # 对应残差块 1x1 卷积输出 channel 个数，3x3 卷积输出 channel 个数翻倍\n",
    "class_num_imagenet = 1000 # for imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Concates(gluon.HybridBlock):\n",
    "    \"\"\"不同 stage 的 feature maps 串接的时候，先经过了一个 1x1 卷积和一个上采样\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super(Concates, self).__init__(*kwargs)\n",
    "        self.concate = nn.HybridSequential(prefix='')\n",
    "        self.concate.add(cbl_gen(channels, (1,1), (1,1), (0,0)))\n",
    "        \n",
    "    def upsample_rept(self, x, stride):\n",
    "        '''\n",
    "        不同的检测层输入堆叠的时候需要上采样，上采样的方式也很简单，\n",
    "        只是将 feature maps 沿着水平和垂直方向 repeat 指定的倍数。\n",
    "        >>> a\n",
    "        array([[0, 1],\n",
    "               [2, 3]])\n",
    "        >>> a.repeat(axis=-1,repeats=1)\n",
    "        array([[0, 1],\n",
    "               [2, 3]])\n",
    "        >>> a.repeat(axis=-1,repeats=2)\n",
    "        array([[0, 0, 1, 1],\n",
    "               [2, 2, 3, 3]])\n",
    "        >>> a.repeat(axis=-1,repeats=2).repeat(axis=-2,repeats=2)\n",
    "        array([[0, 0, 1, 1],\n",
    "               [0, 0, 1, 1],\n",
    "               [2, 2, 3, 3],\n",
    "               [2, 2, 3, 3]])\n",
    "\n",
    "        '''\n",
    "        assert type(x) == mx.ndarray.ndarray.NDArray or type(x) == np.ndarray\n",
    "        return x.repeat(axis=-1, repeats=stride).repeat(axis=-2, repeats=stride)    \n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.concate(x)\n",
    "        x = self.upsample_rept(x, 2)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Detection(gluon.HybridBlock):\n",
    "    '''\n",
    "    检测网络，三个检测网络的结构相同，只是 filter 个数不同,\n",
    "    完全可以使用一个 for 循环实现，\n",
    "    但是需要在倒数第二层引出分支，和前面层的特征合并后，用于前面的检测网络，所以只能分开写\n",
    "\n",
    "    越靠近输入层， feature maps 越大，所以检测网络使用的 channel 相应的较少，防止较大运算量\n",
    "    '''\n",
    "    def __init__(self, channels, classes_num=80, anchors_num=3, **kwargs):\n",
    "        super(Detection, self).__init__(**kwargs)\n",
    "        self.channels=channels\n",
    "        self.anchors_num=anchors_num\n",
    "        self.pred_num=1+4+classes_num\n",
    "        self.body=nn.HybridSequential(prefix='')\n",
    "        self.tip=nn.HybridSequential(prefix='')\n",
    "\n",
    "        for i in range(2):\n",
    "            self.body.add(cbl_gen(channels, (1,1), (1,1), (0,0)))\n",
    "            self.body.add(cbl_gen(channels*2, (3,3), (1,1), (1,1)))\n",
    "            \n",
    "        self.body.add(cbl_gen(channels, (1,1), (1,1), (0,0)))        \n",
    "        self.tip.add(cbl_gen(channels*2, (3,3), (1,1), (1,1)))        \n",
    "\n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.body(x)\n",
    "        return self.tip(x)\n",
    "\n",
    "\n",
    "class Output(gluon.HybridBlock):\n",
    "    \"\"\"YOLOv3 输出\n",
    "    \"\"\"\n",
    "    def __init__(self, anchors, stride, classes_num=80, **kwargs):\n",
    "        super(Output, self).__init__(**kwargs)\n",
    "        self.stride = stride\n",
    "        self.anchors_num = len(anchors) // 2\n",
    "        self.classes_num = classes_num\n",
    "        self.pred_num = 1+4+classes_num\n",
    "        anchors = nd.array(anchors).astype('float32')        \n",
    "        self.anchors = anchors.reshape(1, 1, -1, 2)\n",
    "\n",
    "        self.output = nn.HybridSequential(prefix='')\n",
    "        # 这里是线性激活函数，默认 nn.Conv2D 的 activation=None，两者等效\n",
    "        # 输出 channel 的个数 (4+1+classes)*anchors\n",
    "        self.output.add(nn.Conv2D(self.pred_num*self.anchors_num, (1,1), (1,1), (0,0), groups=1, use_bias=True))        \n",
    "\n",
    "        # offsets will be added to predictions\n",
    "        grid_x = np.arange(128)\n",
    "        grid_y = np.arange(128)\n",
    "        grid_x, grid_y = np.meshgrid(grid_x, grid_y)\n",
    "        # stack to (n, n, 2)\n",
    "        offsets = np.concatenate((grid_x[:, :, np.newaxis], grid_y[:, :, np.newaxis]), axis=-1)\n",
    "        # expand dims to (1, 1, n, n, 2) so it's easier for broadcasting\n",
    "        offsets = np.expand_dims(np.expand_dims(offsets, axis=0), axis=0)\n",
    "        self.offsets = nd.array(offsets)#self.params.get_constant('offset_%d'%(index), offsets)\n",
    "        \n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        pred = self.output(x)\n",
    "        \n",
    "        # prediction flat to (batch, pred per pixel, height * width)\n",
    "        pred = pred.reshape((0, self.anchors_num * self.pred_num, -1))\n",
    "        # transpose to (batch, height * width, num_anchor, num_pred)\n",
    "        pred = pred.transpose(axes=(0, 2, 1)).reshape((0, -1, self.anchors_num, self.pred_num))\n",
    "        # components\n",
    "        raw_box_centers = pred.slice_axis(axis=-1, begin=0, end=2)\n",
    "        raw_box_scales = pred.slice_axis(axis=-1, begin=2, end=4)\n",
    "        objness = pred.slice_axis(axis=-1, begin=4, end=5)\n",
    "        class_pred = pred.slice_axis(axis=-1, begin=5, end=None)\n",
    "\n",
    "        # valid offsets, (1, 1, height, width, 2)\n",
    "        offsets = nd.slice_like(self.offsets, x * 0, axes=(2, 3))\n",
    "        # reshape to (1, height*width, 1, 2)\n",
    "        offsets = offsets.reshape((1, -1, 1, 2))\n",
    "\n",
    "        box_centers = nd.broadcast_add(nd.sigmoid(raw_box_centers), offsets) * self.stride\n",
    "        box_scales = nd.broadcast_mul(nd.exp(raw_box_scales), self.anchors)\n",
    "        confidence = nd.sigmoid(objness)\n",
    "        class_score = nd.broadcast_mul(nd.sigmoid(class_pred), confidence)\n",
    "        wh = box_scales / 2.0\n",
    "        # `corner`: [xmin, ymin, xmax, ymax]\n",
    "        # `center`: [x, y, width, height]\n",
    "        # center to corner\n",
    "        bbox = nd.concat(box_centers - wh, box_centers + wh, dim=-1)\n",
    "\n",
    "        if autograd.is_training():\n",
    "            # during training, we don't need to convert whole bunch of info to detection results\n",
    "            return (bbox.reshape((0, -1, 4)), raw_box_centers, raw_box_scales,\n",
    "                    objness, class_pred, self.anchors, offsets)\n",
    "\n",
    "        # prediction per class\n",
    "        bboxes = nd.tile(bbox, reps=(self.classes_num, 1, 1, 1, 1))\n",
    "        scores = nd.transpose(class_score, axes=(3, 0, 1, 2)).expand_dims(axis=-1)\n",
    "        ids = nd.broadcast_add(scores * 0, F.arange(0, self.classes_num).reshape((0, 1, 1, 1, 1)))\n",
    "        detections = nd.concat(ids, scores, bboxes, dim=-1)\n",
    "        # reshape to (B, xx, 6)\n",
    "        detections = nd.reshape(detections.transpose(axes=(1, 0, 2, 3, 4)), (0, -1, 6))\n",
    "        return detections\n",
    "        \n",
    "        \n",
    "# 三个输出分别使用的检测通道数\n",
    "det_channels = [512, 256, 128]\n",
    "\n",
    "# 这里都进行了反序\n",
    "strides = [32, 16, 8]\n",
    "anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3(gluon.HybridBlock):\n",
    "    \"\"\"生成 YOLOv3 网络，只适用于 Darknet53 ，\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(YOLOv3, self).__init__(**kwargs)\n",
    "        \n",
    "        # 基本网络框架\n",
    "        darknet53 = Darknet53(residual_block_num, darknet_channels)\n",
    "        # residual_block_num = [1, 2, 8, 8, 4] , 每一个残差块的开始都有一个下采样层\n",
    "        feature1_layer = 1 + (1+1) + (1+2) + (1+8)\n",
    "        feature2_layer = feature1_layer + (1+8)\n",
    "        feature3_layer = feature2_layer + (1+4) # 可以直接到末尾，\n",
    "\n",
    "        self.features = nn.HybridSequential(prefix='')\n",
    "        self.features.add(darknet53.features[:feature1_layer])\n",
    "        self.features.add(darknet53.features[feature1_layer:feature2_layer])\n",
    "        self.features.add(darknet53.features[feature2_layer:feature3_layer])\n",
    "\n",
    "        # 从基本网络框架引出的检测网络层，包含输出\n",
    "        self.detection_net = nn.HybridSequential(prefix='')\n",
    "        for det_channel in det_channels:\n",
    "            self.detection_net.add(Detection(det_channel))\n",
    "            \n",
    "        # 串接不同 stage \n",
    "        self.concates = nn.HybridSequential(prefix='')\n",
    "        for det_channel in det_channels[1:]:\n",
    "            self.concates.add(Concates(det_channel))\n",
    "            \n",
    "        # 输出\n",
    "        self.output = nn.HybridSequential(prefix='')\n",
    "        for anchor, stride in zip(anchors, strides):\n",
    "            self.output.add(Output(anchor, stride))\n",
    "\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        \n",
    "        # 先计算出所有 stage 的 features\n",
    "        featuremaps = []\n",
    "        for net in self.features:\n",
    "            x = net(x)\n",
    "            featuremaps.append(x)\n",
    "        \n",
    "        # 反序\n",
    "        featuremaps = featuremaps[::-1]\n",
    "     \n",
    "        output = []\n",
    "        det = nd.array([])\n",
    "        for i in range(len(featuremaps)):\n",
    "            if i == 0:\n",
    "                det = featuremaps[i]\n",
    "            else:\n",
    "                det = self.concates[i-1](det)\n",
    "                det = nd.concat(det, featuremaps[i], dim=1)\n",
    "                \n",
    "            det = self.detection_net[i].body(det)   \n",
    "            out = self.detection_net[i].tip(det)\n",
    "            out = self.output[i](out)\n",
    "            output.append(out)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco 80 个类\n",
    "classes_name = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', \n",
    "                'train', 'truck', 'boat', 'traffic light', 'fire hydrant', \n",
    "                'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', \n",
    "                'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', \n",
    "                'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', \n",
    "                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', \n",
    "                'baseball glove', 'skateboard', 'surfboard', 'tennis racket', \n",
    "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', \n",
    "                'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', \n",
    "                'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', \n",
    "                'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', \n",
    "                'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', \n",
    "                'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 gluoncv 训练好的参数\n",
    "from gluoncv import model_zoo\n",
    "\n",
    "model_net = model_zoo.get_model('yolo3_darknet53_coco', pretrained=True)\n",
    "\n",
    "# 打印参数\n",
    "# print(net.stages[0][0][0].params['darknetv30_conv0_weight'].data())\n",
    "\n",
    "# 保存网络的参数\n",
    "# DarkNet 提取特征层\n",
    "model_net.stages.save_parameters(\"features.params\")\n",
    "\n",
    "# 检测网络\n",
    "for i in range(3):\n",
    "    name = \"body_%d.params\" % (i)\n",
    "    model_net.yolo_blocks[i].body.save_parameters(name)\n",
    "    name = \"tip_%d.params\" % (i)\n",
    "    model_net.yolo_blocks[i].tip.save_parameters(name)\n",
    "\n",
    "# 不同检测网络链接的转换层\n",
    "for i in range(2):\n",
    "    name = \"concate_%d.params\" % (i)\n",
    "    model_net.transitions[i].save_parameters(name)\n",
    "\n",
    "# 输出层\n",
    "for i in range(3):\n",
    "    name = \"outputs_%d.params\" % (i)\n",
    "    model_net.yolo_outputs[i].prediction.save_parameters(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载参数\n",
    "net = YOLOv3()\n",
    "\n",
    "net.features.load_parameters('features.params')\n",
    "\n",
    "# 由于和 gluoncv 的网络结构不同（自己的网络 tip 外面有两层 HybridSequential ），只能分开加载\n",
    "for i in range(3):\n",
    "    name = \"body_%d.params\" % (i)\n",
    "    net.detection_net[i].body.load_parameters(name)\n",
    "    name = \"tip_%d.params\" % (i)\n",
    "    net.detection_net[i].tip[0].load_parameters(name)\n",
    "    \n",
    "for i in range(2):\n",
    "    name = \"concate_%d.params\" % (i)\n",
    "    net.concates[i].concate[0].load_parameters(name)\n",
    "    \n",
    "for i in range(3):\n",
    "    name = \"outputs_%d.params\" % (i)\n",
    "    net.output[i].output[0].load_parameters(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_img(imgname, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    \"\"\"图像 nominalize \n",
    "    测试的时候，transform 不需要额外的处理，只需要归一化以及将 BCWH -> \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : string \n",
    "        待处理的图像名字\n",
    "    mean : iterable of float\n",
    "        Mean pixel values.\n",
    "    std : iterable of float\n",
    "        Standard deviations of pixel values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    网络的输入和 resize 后的图像\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    img = mx.image.imread(imgname)\n",
    "\n",
    "    # 只能在 416*416 时 work\n",
    "    img = mx.image.imresize(img, 416, 416)\n",
    "    orig_img = img.asnumpy().astype('uint8')\n",
    "    img = mx.nd.image.to_tensor(img)\n",
    "    img = mx.nd.image.normalize(img, mean=mean, std=std)\n",
    "    # img.expand_dims(0) 由 CHW -> BCHW\n",
    "    return img.expand_dims(0), orig_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgname = 'giraffe.jpg'\n",
    "imgname = 'person.jpg'\n",
    "imgname = 'dog.jpg'\n",
    "imgname = 'horses.jpg'\n",
    "imgname = 'kite.jpg'\n",
    "x, img = preprocessing_img(imgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxoutput = 100\n",
    "class_threshold = 0.5\n",
    "\n",
    "def clean_output(data, maxoutput, class_threshold):\n",
    "    \"\"\"找到最大输出个数的值，并删掉低概率的值\n",
    "    data: Ndarray\n",
    "        待处理数据 \n",
    "    maxoutput: int\n",
    "        最大输出个数\n",
    "    class_thread: float\n",
    "        类别概率阈值，低于该值的不予输出\n",
    "    \"\"\"\n",
    "    output = nd.array([[0,0,0,0,0,0]])\n",
    "\n",
    "    # 先从每个输出中取出最大个数的输出，并拼接\n",
    "    for i in range(len(data)):\n",
    "        # 压缩维度，自动去掉值为 1 的维度\n",
    "        res = np.squeeze(data[i])\n",
    "\n",
    "        index = res[:,1].argsort()\n",
    "        index = index[::-1][:maxoutput]\n",
    "        otmp = res[index,:]\n",
    "\n",
    "        output = nd.concat(output,otmp,dim=0)\n",
    "\n",
    "    output = output.asnumpy()\n",
    "    keepindex = np.where(output[:,1] >= class_threshold)[0]\n",
    "    output = output[keepindex,:]\n",
    "    if len(output) > maxoutput:\n",
    "        index = output[:,1].argsort()\n",
    "        index = index[::-1][:maxoutput]\n",
    "        output = output[index,:]\n",
    "    return output\n",
    "\n",
    "# bbox 是 (xmin,ymin,xmax,ymax) 格式\n",
    "output = clean_output(y, maxoutput, class_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  classify_ouput(data):\n",
    "    \"\"\"将输出分类，便于分类别进行 NMS\n",
    "    data : numpy array\n",
    "    \n",
    "    输出 list \n",
    "    \"\"\"\n",
    "    # 按照类别进行排序\n",
    "    index = data[:,0].argsort()\n",
    "    data = data[index,:]\n",
    "    \n",
    "    split_index = []\n",
    "    clas = data[0,0]\n",
    "    index = 0\n",
    "    for i in range(1, len(data)):\n",
    "        index += 1\n",
    "        if clas != data[i,0]:\n",
    "            split_index.append(index)\n",
    "            clas = data[i,0]    \n",
    "\n",
    "    return np.split(data, split_index)\n",
    "\n",
    "\n",
    "classoutput = classify_ouput(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(bbox_a, bbox_b):\n",
    "    \"\"\"计算两组 bounding boxes 的 Intersection-Over-Union(IOU)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox_a : numpy.ndarray\n",
    "        shape (M, 4) . bbox 格式 (xmin,ymin,xmax,ymax)\n",
    "    bbox_b : numpy.ndarray\n",
    "        shape (N, 4) . bbox 格式 (xmin,ymin,xmax,ymax)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    二维数组 shape (M,N) ，其中任意一个元素 (i,j) 表示 bboxa[i] 和 bboxb[j] 的 IoU\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if bbox_a.shape[1] < 4 or bbox_b.shape[1] < 4:\n",
    "        raise IndexError(\"Bounding boxes axis 1 must have at least length 4\")\n",
    "\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    br = np.minimum(bbox_a[:, None, 2:4], bbox_b[:, 2:4])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:4] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:4] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "\n",
    "# 依据 score 排序，从 score 最高第一个 box 开始，所有与该 box IOU 大于指定阈值的\n",
    "# box 都会被删掉；同时把 box 加入到最终的队列中，并从原 list 中删除。\n",
    "# 接着用队列中剩下的 score 最高的 box 去抑制队列中剩余的 box\n",
    "\n",
    "# 可以先删掉 score 比较低的 box\n",
    "def non_max_suppression(boxes, scores, threshold=0.7):\n",
    "    \"\"\"Performs non-maximum suppression and returns indices of kept boxes.\n",
    "    boxes: [N, (y1, x1, y2, x2)]. Notice that (y2, x2) lays outside the box.\n",
    "    scores: 1-D array of box scores.\n",
    "    threshold: Float. IoU threshold to use for filtering. 一般为 0.7\n",
    "    \"\"\"\n",
    "    assert boxes.shape[0] > 0\n",
    "    if boxes.dtype.kind != \"f\":\n",
    "        boxes = boxes.astype(np.float32)\n",
    "\n",
    "    # scores 从大到小排序\n",
    "    ixs = scores.argsort()[::-1]\n",
    "\n",
    "    pick = []\n",
    "    while len(ixs) > 0:\n",
    "        # Pick top box and add its index to the list\n",
    "        # 每次都选择队列中 score 最高的 box ，并用他抑制队列中剩余的 box\n",
    "        i = ixs[0]\n",
    "        pick.append(i)\n",
    "        # Compute IoU of the picked box with the rest\n",
    "        iou = compute_iou(boxes[i][np.newaxis,:], boxes[ixs[1:]])\n",
    "        # Identify boxes with IoU over the threshold. This\n",
    "        # returns indices into ixs[1:], so add 1 to get\n",
    "        # indices into ixs.\n",
    "        remove_ixs = np.where(iou > threshold)[1] + 1\n",
    "        # Remove indices of the picked and overlapped boxes.\n",
    "        # 所有与 score 最高的 box IoU 大于阈值的 box 都从队列中移除\n",
    "        ixs = np.delete(ixs, remove_ixs)\n",
    "        ixs = np.delete(ixs, 0)\n",
    "    return np.array(pick, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classnms(data, threshold=0.7):\n",
    "    \"\"\"将输出结果分类别进行 NMS ，输出处理后的结果\"\"\"\n",
    "    \n",
    "    output = []\n",
    "    for i in range(len(data)):\n",
    "        bbox = data[i][:,2:]\n",
    "        score = data[i][:,1]\n",
    "        keepindex = non_max_suppression(bbox, score, threshold)\n",
    "        if i == 0:\n",
    "            output = data[i][keepindex,:]\n",
    "        else:\n",
    "            output = np.row_stack([output,data[i][keepindex,:]])\n",
    "\n",
    "    return output\n",
    "        \n",
    "plotoutput = classnms(classoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plotoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv import utils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "bounding_boxs = plotoutput[:,2:]\n",
    "scores = plotoutput[:,1]\n",
    "class_IDs = plotoutput[:,0]\n",
    "ax = utils.viz.plot_bbox(img, bounding_boxs, scores,\n",
    "                         class_IDs, class_names=classes_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def anchor_iou(gt_wh, anchors):\n",
    "    \n",
    "    \"\"\"只用于 gt_box 和 anchor 的 IOU 计算\n",
    "    gt_wh : 一个 groundtruth box 的宽和高，如 [95, 213]\n",
    "    anchors : 一系列 anchors ，例如 [116, 90, 156, 198, 373, 326]\n",
    "    \n",
    "    \n",
    "    >>> boxes=np.array([[12,34],[98,57]])\n",
    "    >>> np.maximum(fs[0], boxes[:,0])\n",
    "    array([56, 98])\n",
    "    >>> fs\n",
    "    array([56, 56])\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(anchors) % 2 == 0\n",
    "    \n",
    "    anchors = anchors.reshape((-1,2))\n",
    "    inter_w = np.minimum(gt_wh[0], anchors[:,0])\n",
    "    inter_h = np.minimum(gt_wh[1], anchors[:,1])\n",
    "    \n",
    "    inter = inter_w * inter_h\n",
    "    union = gt_wh[0] * gt_wh[1] + anchors[:, 0] * anchors[:, 1] - inter\n",
    "    \n",
    "    iou = inter / union   \n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "def gen_target(gt_box, obj_class, class_num, image_size, anchors, featuremap_size):\n",
    "    \"\"\"依据 anchors 将 gt_box 转换成 target\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_box : list\n",
    "        存储目标真实位置的列表，box 存储方式 (x,y,w,h) ， list 维度 (B,n,4) \n",
    "    anchors： list of list\n",
    "        不同 stage 的 anchor 组成的 list. [[...], [...], ...]\n",
    "    featuremap_size : list of list\n",
    "        不同 stage 的 feature maps spatial size\n",
    "        \n",
    "    return\n",
    "    ------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #assert len(anchors) == len(featuremap_size)\n",
    "    #batch_size = len(get_box[0])\n",
    "    target = np.zeros((featuremap_size[0], featuremap_size[1], len(anchors) // 2, 4))\n",
    "    print(target.shape)\n",
    "    \n",
    "    anchor_index = anchor_iou(gt_box[2:], anchors).argmax(axis=0)\n",
    "    spatial_index = (gt_box[:2] / image_size * featuremap_size).astype('int')\n",
    "    print(spatial_index)\n",
    "    \n",
    "    for i in range(len(gt_box)//4):\n",
    "        target[spatial_index[0], spatial_index[1], anchor_index[i], 0:2] = gt_box[0:2]\n",
    "        target[spatial_index[0], spatial_index[1], anchor_index[i], 2:4] = gt_box[2:4]\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box = np.array([12, 43, 362, 356])\n",
    "image_size = np.array([448, 448])\n",
    "featuremap_size = np.array([56, 56])\n",
    "anchors = np.array([116, 90, 156, 198, 373, 326])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = gen_target(gt_box, 1, 10, image_size, anchors, featuremap_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target[1,5,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_iou([95,213], anchors).argmax(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3 LOSS 共包括 `obj_loss, center_loss, scale_loss, cls_loss` 四部分，\n",
    "\n",
    "1. scale_loss 使用 gluon.loss.L1Loss() 计算 loss\n",
    "1. obj_loss, center_loss, cls_loss 使用 gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False) 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolov3_loss():\n",
    "    def __init__(self):\n",
    "        \n",
    "        obj_loss = 0\n",
    "        center_loss = 0\n",
    "        scale_loss = 0\n",
    "        cls_loss = 0\n",
    "    \n",
    "        return obj_loss, center_loss, scale_loss, cls_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
